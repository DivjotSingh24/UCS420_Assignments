{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_7Pj1-LRgmL"
      },
      "source": [
        "Q1. Write a unique paragraph (5-6 sentences) about your favorite topic (e.g., sports, technology, food, books, etc.).\n",
        "\n",
        "1. Convert text to lowercase and remove punctuaƟon.\n",
        "2. Tokenize the text into words and sentences.\n",
        "3. Remove stopwords (using NLTK's stopwords list).\n",
        "4. Display word frequency distribuƟon (excluding stopwords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cVZKI05TR6eJ",
        "outputId": "4e85117d-c4ee-4e8d-cb04-59cb2880fd30"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "import re\n",
        "import string\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "from nltk.stem import PorterStemmer, LancasterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from collections import Counter\n",
        "\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hS4norwQU7v4",
        "outputId": "52addaf2-2f7f-4cdf-baec-658d8457fe61"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cleaned Text:\n",
            "last weekend a squirrel stole my sandwich while i was sitting in the park\n",
            "it climbed down a tree grabbed the sandwich with its tiny paws and ran off like a seasoned thief\n",
            "people around me were laughing while i sat in shock wondering what just happened\n",
            "i guess nature has its own sense of humor\n",
            "ever since then i bring extra food just in case the squirrel comes back for round two\n",
            "\n",
            "Tokens:\n",
            "['last', 'weekend', 'a', 'squirrel', 'stole', 'my', 'sandwich', 'while', 'i', 'was', 'sitting', 'in', 'the', 'park', 'it', 'climbed', 'down', 'a', 'tree', 'grabbed', 'the', 'sandwich', 'with', 'its', 'tiny', 'paws', 'and', 'ran', 'off', 'like', 'a', 'seasoned', 'thief', 'people', 'around', 'me', 'were', 'laughing', 'while', 'i', 'sat', 'in', 'shock', 'wondering', 'what', 'just', 'happened', 'i', 'guess', 'nature', 'has', 'its', 'own', 'sense', 'of', 'humor', 'ever', 'since', 'then', 'i', 'bring', 'extra', 'food', 'just', 'in', 'case', 'the', 'squirrel', 'comes', 'back', 'for', 'round', 'two']\n",
            "\n",
            "Filtered Words:\n",
            "['last', 'weekend', 'squirrel', 'stole', 'sandwich', 'sitting', 'park', 'climbed', 'tree', 'grabbed', 'sandwich', 'tiny', 'paws', 'ran', 'like', 'seasoned', 'thief', 'people', 'around', 'laughing', 'sat', 'shock', 'wondering', 'happened', 'guess', 'nature', 'sense', 'humor', 'ever', 'since', 'bring', 'extra', 'food', 'case', 'squirrel', 'comes', 'back', 'round', 'two']\n",
            "Word Frequency Distribution (Excluding Stopwords):\n",
            "Counter({'squirrel': 2, 'sandwich': 2, 'last': 1, 'weekend': 1, 'stole': 1, 'sitting': 1, 'park': 1, 'climbed': 1, 'tree': 1, 'grabbed': 1, 'tiny': 1, 'paws': 1, 'ran': 1, 'like': 1, 'seasoned': 1, 'thief': 1, 'people': 1, 'around': 1, 'laughing': 1, 'sat': 1, 'shock': 1, 'wondering': 1, 'happened': 1, 'guess': 1, 'nature': 1, 'sense': 1, 'humor': 1, 'ever': 1, 'since': 1, 'bring': 1, 'extra': 1, 'food': 1, 'case': 1, 'comes': 1, 'back': 1, 'round': 1, 'two': 1})\n"
          ]
        }
      ],
      "source": [
        "text = \"\"\"Last weekend, a squirrel stole my sandwich while I was sitting in the park.\n",
        "It climbed down a tree, grabbed the sandwich with its tiny paws, and ran off like a seasoned thief.\n",
        "People around me were laughing while I sat in shock, wondering what just happened.\n",
        "I guess nature has its own sense of humor.\n",
        "Ever since then, I bring extra food just in case the squirrel comes back for round two.\"\"\"\n",
        "\n",
        "# 1. Convert text to lowercase and remove punctuation\n",
        "text_lower = text.lower()\n",
        "text_clean = text_lower.translate(str.maketrans('', '', string.punctuation))\n",
        "print(\"Cleaned Text:\")\n",
        "print(text_clean)\n",
        "\n",
        "# 2. Tokenize the text into words and sentence\n",
        "words = word_tokenize(text_clean)\n",
        "sentences = sent_tokenize(text)\n",
        "print(\"\\nTokens:\")\n",
        "print(words)\n",
        "\n",
        "# 3. Remove stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "filtered_words = [word for word in words if word not in stop_words]\n",
        "print(\"\\nFiltered Words:\")\n",
        "print(filtered_words)\n",
        "\n",
        "# 4. Display word frequency distribution\n",
        "word_freq = Counter(filtered_words)\n",
        "print(\"Word Frequency Distribution (Excluding Stopwords):\")\n",
        "print(word_freq)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywe09RRBVG5f"
      },
      "source": [
        "Q2: Stemming and LemmaƟzaƟon\n",
        "\n",
        "1. Take the tokenized words from QuesƟon 1 (aŌer stopword removal).\n",
        "2. Apply stemming using NLTK's PorterStemmer and LancasterStemmer.\n",
        "3. Apply lemmaƟzaƟon using NLTK's WordNetLemmaƟzer.\n",
        "4. Compare and display results of both techniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4efGkU6VLNe",
        "outputId": "02c46c93-6ad1-4129-a2a5-0e1ea0daf76a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Stemming (PorterStemmer): ['last', 'weekend', 'squirrel', 'stole', 'sandwich', 'sit', 'park', 'climb', 'tree', 'grab', 'sandwich', 'tini', 'paw', 'ran', 'like', 'season', 'thief', 'peopl', 'around', 'laugh', 'sat', 'shock', 'wonder', 'happen', 'guess', 'natur', 'sens', 'humor', 'ever', 'sinc', 'bring', 'extra', 'food', 'case', 'squirrel', 'come', 'back', 'round', 'two']\n",
            "\n",
            "Stemming (LancasterStemmer): ['last', 'weekend', 'squirrel', 'stol', 'sandwich', 'sit', 'park', 'climb', 'tre', 'grab', 'sandwich', 'tiny', 'paw', 'ran', 'lik', 'season', 'thief', 'peopl', 'around', 'laugh', 'sat', 'shock', 'wond', 'hap', 'guess', 'nat', 'sens', 'hum', 'ev', 'sint', 'bring', 'extr', 'food', 'cas', 'squirrel', 'com', 'back', 'round', 'two']\n",
            "\n",
            "Lemmatization: ['last', 'weekend', 'squirrel', 'stole', 'sandwich', 'sitting', 'park', 'climbed', 'tree', 'grabbed', 'sandwich', 'tiny', 'paw', 'ran', 'like', 'seasoned', 'thief', 'people', 'around', 'laughing', 'sat', 'shock', 'wondering', 'happened', 'guess', 'nature', 'sense', 'humor', 'ever', 'since', 'bring', 'extra', 'food', 'case', 'squirrel', 'come', 'back', 'round', 'two']\n"
          ]
        }
      ],
      "source": [
        "porter_stemmer = PorterStemmer()\n",
        "lancaster_stemmer = LancasterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "porter_stemmed = [porter_stemmer.stem(word) for word in filtered_words]\n",
        "lancaster_stemmed = [lancaster_stemmer.stem(word) for word in filtered_words]\n",
        "lemmatized = [lemmatizer.lemmatize(word) for word in filtered_words]\n",
        "print(\"\\nStemming (PorterStemmer):\", porter_stemmed)\n",
        "print(\"\\nStemming (LancasterStemmer):\", lancaster_stemmed)\n",
        "print(\"\\nLemmatization:\", lemmatized)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2eOd2mYXVYWS"
      },
      "source": [
        "Q3. Regular Expressions and Text Spliƫng\n",
        "\n",
        "Take their original text from QuesƟon 1.\n",
        "Use regular expressions to: a. Extract all words with more than 5 leƩers. b. Extract all numbers (if any exist in their text). c. Extract all capitalized words.\n",
        "Use text spliƫng techniques to: a. Split the text into words containing only alphabets (removing digits and special characters). b. Extract words starƟng with a vowel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZvm9HRzVeIV",
        "outputId": "f9ffab13-15ba-4b93-9115-1cc9c4dff4c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Words with more than 5 letters: ['weekend', 'squirrel', 'sandwich', 'sitting', 'climbed', 'grabbed', 'sandwich', 'seasoned', 'people', 'around', 'laughing', 'wondering', 'happened', 'nature', 'squirrel']\n",
            "\n",
            "Numbers found in text: []\n",
            "\n",
            "Capitalized words: []\n",
            "\n",
            "Alphabetic words: ['last', 'weekend', 'a', 'squirrel', 'stole', 'my', 'sandwich', 'while', 'i', 'was', 'sitting', 'in', 'the', 'park', 'it', 'climbed', 'down', 'a', 'tree', 'grabbed', 'the', 'sandwich', 'with', 'its', 'tiny', 'paws', 'and', 'ran', 'off', 'like', 'a', 'seasoned', 'thief', 'people', 'around', 'me', 'were', 'laughing', 'while', 'i', 'sat', 'in', 'shock', 'wondering', 'what', 'just', 'happened', 'i', 'guess', 'nature', 'has', 'its', 'own', 'sense', 'of', 'humor', 'ever', 'since', 'then', 'i', 'bring', 'extra', 'food', 'just', 'in', 'case', 'the', 'squirrel', 'comes', 'back', 'for', 'round', 'two']\n",
            "\n",
            "Words starting with a vowel: ['a', 'i', 'in', 'it', 'a', 'its', 'and', 'off', 'a', 'around', 'i', 'in', 'i', 'its', 'own', 'of', 'ever', 'i', 'extra', 'in']\n"
          ]
        }
      ],
      "source": [
        "long_words = re.findall(r'\\b\\w{6,}\\b', text_clean)\n",
        "print(\"\\nWords with more than 5 letters:\", long_words)\n",
        "\n",
        "numbers = re.findall(r'\\b\\d+\\b', text_clean)\n",
        "print(\"\\nNumbers found in text:\", numbers)\n",
        "\n",
        "capitalized_words = re.findall(r'\\b[A-Z][a-z]*\\b', text_clean)\n",
        "print(\"\\nCapitalized words:\", capitalized_words)\n",
        "\n",
        "alphabetic_words = re.findall(r'\\b[a-zA-Z]+\\b', text_clean)\n",
        "print(\"\\nAlphabetic words:\", alphabetic_words)\n",
        "\n",
        "vowel_words = re.findall(r'\\b[aeiouAEIOU]\\w*\\b', text_clean)\n",
        "print(\"\\nWords starting with a vowel:\", vowel_words)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AMrICNmsVq57"
      },
      "source": [
        "Q4. Custom TokenizaƟon & Regex-based Text Cleaning\n",
        "\n",
        "Take original text from QuesƟon 1.\n",
        "Write a custom tokenizaƟon funcƟon that: a. Removes punctuaƟon and special symbols, but keeps contracƟons (e.g., \"isn't\" should not be split into \"is\" and \"n't\"). b. Handles hyphenated words as a single token (e.g., \"state-of-the-art\" remains a single token). c. Tokenizes numbers separately but keeps decimal numbers intact (e.g., \"3.14\" should remain as is).\n",
        "Use Regex SubsƟtuƟons (re.sub) to: a. Replace email addresses with '' placeholder. b. Replace URLs with '' placeholder. c. Replace phone numbers (formats: 123-456-7890 or +91 9876543210) with '' placeholder."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKDZ8_paVuq9",
        "outputId": "0887f46a-5949-471f-f0c2-29ee081e447a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Custom Tokenized Text: ['last', 'weekend', 'a', 'squirrel', 'stole', 'my', 'sandwich', 'while', 'i', 'was', 'sitting', 'in', 'the', 'park', 'it', 'climbed', 'down', 'a', 'tree', 'grabbed', 'the', 'sandwich', 'with', 'its', 'tiny', 'paws', 'and', 'ran', 'off', 'like', 'a', 'seasoned', 'thief', 'people', 'around', 'me', 'were', 'laughing', 'while', 'i', 'sat', 'in', 'shock', 'wondering', 'what', 'just', 'happened', 'i', 'guess', 'nature', 'has', 'its', 'own', 'sense', 'of', 'humor', 'ever', 'since', 'then', 'i', 'bring', 'extra', 'food', 'just', 'in', 'case', 'the', 'squirrel', 'comes', 'back', 'for', 'round', 'two']\n",
            "\n",
            "Text with Emails, URLs, and Phone Numbers Replaced:\n",
            "Last weekend, a squirrel stole my sandwich while I was sitting in the park.\n",
            "It climbed down a tree, grabbed the sandwich with its tiny paws, and ran off like a seasoned thief.\n",
            "People around me were laughing while I sat in shock, wondering what just happened.\n",
            "I guess nature has its own sense of humor.\n",
            "Ever since then, I bring extra food just in case the squirrel comes back for round two.\n"
          ]
        }
      ],
      "source": [
        "def custom_tokenizer(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'[^\\w\\s\\'-]', '', text)\n",
        "    return word_tokenize(text)\n",
        "\n",
        "custom_tokens = custom_tokenizer(text)\n",
        "print(\"\\nCustom Tokenized Text:\", custom_tokens)\n",
        "\n",
        "text_with_emails = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,7}\\b', '<EMAIL>', text)\n",
        "\n",
        "text_with_urls = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '<URL>', text_with_emails)\n",
        "\n",
        "text_cleaned = re.sub(r'(\\+?\\d{1,2}\\s?)?(\\(?\\d{3}\\)?[-.\\s]?\\d{3}[-.\\s]?\\d{4})', '<PHONE>', text_with_urls)\n",
        "\n",
        "print(\"\\nText with Emails, URLs, and Phone Numbers Replaced:\")\n",
        "print(text_cleaned)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}